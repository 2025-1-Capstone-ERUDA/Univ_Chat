import json
import requests
from bs4 import BeautifulSoup
import time

# âœ… 1. JSON íŒŒì¼ì—ì„œ URL ì½ê¸°
with open("data/crawler_url.json", "r", encoding="utf-8") as file:
    data = json.load(file)

# âœ… 2. "urls" ë˜ëŠ” "url" í‚¤ í™•ì¸
if "urls" in data:
    url_list = data["urls"]
elif "url" in data:
    url_list = [data["url"]]
else:
    print("âš ï¸ ì˜¤ë¥˜: 'url' ë˜ëŠ” 'urls' í‚¤ê°€ JSON íŒŒì¼ì— ì—†ìŠµë‹ˆë‹¤.")
    url_list = []

visited_posts = set()  # âœ… 5. ì¤‘ë³µ ê²Œì‹œë¬¼ ë°©ë¬¸ ë°©ì§€

# âœ… 3. ì—¬ëŸ¬ ê°œì˜ ê³µì§€ì‚¬í•­ URL ì²˜ë¦¬
for site in url_list:
    url = site["url"]
    crawl_delay = site.get("crawl_delay", 5)  # siteì—ì„œ êº¼ë‚´ì•¼ í•´!

    print(f"ğŸ” {url} í¬ë¡¤ë§ ì¤‘...")

    try:
        response = requests.get(url, timeout=site.get("crawl_timeout", 30))  # siteì—ì„œ timeoutë„ êº¼ë‚´ê³ 
        response.raise_for_status()

        soup = BeautifulSoup(response.text, "html.parser")

        posts = soup.select("table.board-table td.title a")  

        if not posts:
            print(f"âš ï¸ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. HTML êµ¬ì¡° í™•ì¸ í•„ìš”!")
            print(soup.prettify())
            continue

        for post in posts:
            title = post.text.strip()
            link = post["href"]

            if not link.startswith("http"):
                link = "https://cse.kangwon.ac.kr/cse/community/" + link

            if link in visited_posts:
                print(f"âš ï¸ ì´ë¯¸ ë°©ë¬¸í•œ ê²Œì‹œë¬¼: {title} (ê±´ë„ˆëœ€)")
                continue

            print(f"ğŸ“Œ ì œëª©: {title}, ë§í¬: {link}")
            visited_posts.add(link)

        time.sleep(crawl_delay)

    except requests.RequestException as e:
        print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {e}")

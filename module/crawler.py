import json
import requests
from bs4 import BeautifulSoup
import time

# âœ… 1. JSON íŒŒì¼ì—ì„œ URL ì½ê¸°
with open("data/crawler_url.json", "r", encoding="utf-8") as file:
    data = json.load(file)

# âœ… 2. "urls" ë˜ëŠ” "url" í‚¤ í™•ì¸
if "urls" in data:
    url_list = data["urls"]
elif "url" in data:
    url_list = [data["url"]]
else:
    print("âš ï¸ ì˜¤ë¥˜: 'url' ë˜ëŠ” 'urls' í‚¤ê°€ JSON íŒŒì¼ì— ì—†ìŠµë‹ˆë‹¤.")
    url_list = []

visited_posts = set()  # âœ… 5. ì¤‘ë³µ ê²Œì‹œë¬¼ ë°©ë¬¸ ë°©ì§€

# âœ… 3. ì—¬ëŸ¬ ê°œì˜ ê³µì§€ì‚¬í•­ URL ì²˜ë¦¬
for url in url_list:
    crawl_delay = data.get("crawl_delay", 5)  # í¬ë¡¤ë§ ê°„ê²© ê¸°ë³¸ê°’ 5ì´ˆ

    print(f"ğŸ” {url} í¬ë¡¤ë§ ì¤‘...")

    try:
        response = requests.get(url, timeout=data.get("crawl_timeout", 30))
        response.raise_for_status()  # ìš”ì²­ ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ì²˜ë¦¬

        soup = BeautifulSoup(response.text, "html.parser")

        # âœ… 4. ê²Œì‹œê¸€ ëª©ë¡ í¬ë¡¤ë§ (ì œëª© & ë§í¬)
        posts = soup.select("table.board-table td.title a")  

        if not posts:
            print(f"âš ï¸ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. HTML êµ¬ì¡° í™•ì¸ í•„ìš”!")
            print(soup.prettify())  # ğŸ”¥ í˜ì´ì§€ ì „ì²´ HTML ì¶œë ¥í•´ì„œ êµ¬ì¡° í™•ì¸
            continue

        for post in posts:
            title = post.text.strip()  # ê²Œì‹œê¸€ ì œëª©
            link = post["href"]  # ìƒëŒ€ URL

            # âœ… 5. ë§í¬ê°€ ìƒëŒ€ê²½ë¡œë¼ë©´ ì ˆëŒ€ê²½ë¡œë¡œ ë³€í™˜
            if not link.startswith("http"):  
                link = "https://cse.kangwon.ac.kr/cse/community/" + link

            # âœ… 6. ì¤‘ë³µ ê²Œì‹œë¬¼ ë°©ë¬¸ ë°©ì§€
            if link in visited_posts:
                print(f"âš ï¸ ì´ë¯¸ ë°©ë¬¸í•œ ê²Œì‹œë¬¼: {title} (ê±´ë„ˆëœ€)")
                continue

            print(f"ğŸ“Œ ì œëª©: {title}, ë§í¬: {link}")
            visited_posts.add(link)  # ë°©ë¬¸í•œ ê²Œì‹œë¬¼ ê¸°ë¡

        time.sleep(crawl_delay)  # í¬ë¡¤ë§ ê°„ê²© ì¡°ì ˆ
    
    except requests.RequestException as e:
        print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {e}")
